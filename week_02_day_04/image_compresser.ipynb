{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip freeze | grep albumentations || pip install albumentations"
      ],
      "metadata": {
        "id": "VVz_O5vYafp3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!if [ ! -d \"./datasets/DIV2K_train_HR\" ]; then \\\n",
        "    wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip -P ./datasets && \\\n",
        "    unzip ./datasets/DIV2K_train_HR.zip -d ./datasets; \\\n",
        "fi\n",
        "\n",
        "!if [ ! -d \"./datasets/DIV2K_valid_LR_x8\" ]; then \\\n",
        "    wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_x8.zip -P ./datasets && \\\n",
        "    unzip ./datasets/DIV2K_valid_LR_x8.zip -d ./datasets; \\\n",
        "fi"
      ],
      "metadata": {
        "id": "isvCcQ8gV6i5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from math import log2\n",
        "import numpy as np\n",
        "import torch \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from torchvision.models import vgg19\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations import Resize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6R3gfAQmT5WT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = './datasets/DIV2K_train_HR/'\n",
        "VAL_PATH = './datasets/DIV2K_valid_LR_x8/'\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = True\n",
        "CHECKPOINT_GEN = 'gen.pth.tar'\n",
        "CHECKPOINT_DISC = 'disc.pth.tar'\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "LEARNING_RATE = 1e-4\n",
        "START_EPOCHS = 1\n",
        "NUM_EPOCHS = 500\n",
        "BATCH_SIZE = 1\n",
        "NUM_WORKERS = 4\n",
        "HIGH_RES = 256\n",
        "RATIO = 8\n",
        "LOW_RES = HIGH_RES // RATIO\n",
        "IMG_CHANNELS = 3"
      ],
      "metadata": {
        "id": "sVl2ERPSTObL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_res_transform = A.Compose([\n",
        "  A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "  ToTensorV2(),\n",
        "])\n",
        "low_res_transform = A.Compose([\n",
        "  A.Resize(width=LOW_RES, height=LOW_RES, interpolation=Image.BICUBIC),\n",
        "  A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
        "  ToTensorV2(),\n",
        "])\n",
        "both_transforms = A.Compose([\n",
        "  A.RandomCrop(width=HIGH_RES, height=HIGH_RES),\n",
        "  A.HorizontalFlip(p=0.5),\n",
        "  A.RandomRotate90(p=0.5),\n",
        "])"
      ],
      "metadata": {
        "id": "nDaRnrxbaVpM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(critic, real, fake, device):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake.detach() * (1 - alpha)\n",
        "    interpolated_images.requires_grad_(True)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def plot_examples(low_res_folder, gen):\n",
        "    os.system(\"rm saved/*\")\n",
        "    files = os.listdir(low_res_folder)\n",
        "    np.random.shuffle(files)\n",
        "    gen.eval()\n",
        "    for file in files[:10]:\n",
        "        image = Image.open(low_res_folder + file)\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                upscaled_img = gen(\n",
        "                    test_transform(image=np.asarray(image))[\"image\"]\n",
        "                    .unsqueeze(0)\n",
        "                    .to(DEVICE)\n",
        "                )\n",
        "            save_image(upscaled_img * 0.5 + 0.5, f\"saved/{file}\")\n",
        "        except : print('Memory insufficient for that image')\n",
        "    gen.train()\n",
        "\n",
        "def compress(img: np.ndarray, ratio:int):\n",
        "    resized = Resize(width=img.shape[1]//ratio, height=img.shape[0]//ratio, interpolation=Image.BICUBIC)(image=img)[\"image\"]\n",
        "    return resized"
      ],
      "metadata": {
        "id": "B_4iuPuZ-qHE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HhmrPnlhLems"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, root_dir):\n",
        "    super(MyDataset, self).__init__()\n",
        "    self.data = []\n",
        "    self.root_dir = root_dir\n",
        "    self.data = [os.path.join(root_dir, fl) for fl in os.listdir(root_dir)]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img_file = self.data[index]\n",
        "    image = np.array(Image.open(img_file))\n",
        "    image = both_transforms(image=image)['image']\n",
        "    high_res = high_res_transform(image=image)['image']\n",
        "    low_res = low_res_transform(image=image)['image']\n",
        "    return high_res, low_res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(\n",
        "    self, \n",
        "    in_channels, \n",
        "    out_channels, \n",
        "    discriminator=False, \n",
        "    use_act=True, \n",
        "    use_bn=True, \n",
        "    **kwargs,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.use_act = use_act\n",
        "    self.cnn = nn.Conv2d(in_channels, out_channels, **kwargs, bias=(not use_bn))\n",
        "    self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
        "    self.act = (\n",
        "      nn.LeakyReLU(0.2, inplace=True) \n",
        "      if discriminator \n",
        "      else nn.PReLU(num_parameters=out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.cnn(x)\n",
        "    out = self.bn(out)\n",
        "    if self.use_act:\n",
        "      out = self.act(out)\n",
        "    return out\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super().__init__()\n",
        "    self.block1 = ConvBlock(\n",
        "      in_channels,\n",
        "      in_channels,\n",
        "      kernel_size=3,\n",
        "      stride=1,\n",
        "      padding=1,\n",
        "    )\n",
        "    self.block2 = ConvBlock(\n",
        "      in_channels,\n",
        "      in_channels,\n",
        "      kernel_size=3,\n",
        "      stride=1,\n",
        "      padding=1,\n",
        "      use_act=False,\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.block1(x)\n",
        "    out = self.block2(out)\n",
        "    return out+x\n",
        "\n",
        "class UnsampleBlock(nn.Module):\n",
        "  def __init__(self, in_channels, scale_factor):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(\n",
        "      in_channels,\n",
        "      in_channels * scale_factor**2,\n",
        "      kernel_size=3,\n",
        "      stride=1,\n",
        "      padding=1,\n",
        "    )\n",
        "    self.ps = nn.PixelShuffle(upscale_factor=scale_factor)\n",
        "    self.act = nn.PReLU(num_parameters=in_channels)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = self.ps(out)\n",
        "    out = self.act(out)\n",
        "    return out\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_channels=3, num_channels=64, num_blocks=16, ratio=4):\n",
        "    super().__init__()\n",
        "    self.initial = ConvBlock(\n",
        "      in_channels, \n",
        "      num_channels, \n",
        "      kernel_size=9, \n",
        "      stride=1, \n",
        "      padding=4, \n",
        "      use_bn=False\n",
        "    )\n",
        "    self.residuals = nn.Sequential(\n",
        "      *[ResidualBlock(num_channels) for _ in range(num_blocks)]\n",
        "    )\n",
        "    self.convblock = ConvBlock(\n",
        "      num_channels, \n",
        "      num_channels, \n",
        "      kernel_size=3, \n",
        "      stride=1, \n",
        "      padding=1, \n",
        "      use_act=False\n",
        "    )\n",
        "    self.unsamples = nn.Sequential(\n",
        "      *[UnsampleBlock(num_channels, 2) for _ in range(int(log2(ratio)))]\n",
        "    )\n",
        "    self.final = nn.Conv2d(\n",
        "      num_channels,\n",
        "      in_channels,\n",
        "      kernel_size=9,\n",
        "      stride=1,\n",
        "      padding=4\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    initial = self.initial(x)\n",
        "    x = self.residuals(initial)\n",
        "    x = self.convblock(x) + initial\n",
        "    x = self.unsamples(x)\n",
        "    return torch.tanh(self.final(x))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels=3, features=[64, 64, 128, 128, 256, 256, 512, 512]):\n",
        "    super().__init__()\n",
        "    blocks = []\n",
        "    for idx, feature in enumerate(features):\n",
        "      blocks.append(\n",
        "        ConvBlock(\n",
        "          in_channels, \n",
        "          feature, \n",
        "          kernel_size=3, \n",
        "          stride=(1 + idx%2), \n",
        "          padding=1, \n",
        "          discriminator=True,\n",
        "          use_act=True,\n",
        "          use_bn=idx\n",
        "        )\n",
        "      )\n",
        "      in_channels = feature\n",
        "    \n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.AdaptiveAvgPool2d((6, 6)),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(512*6*6, 1024),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      nn.Linear(1024, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.blocks(x)\n",
        "    return self.classifier(x)"
      ],
      "metadata": {
        "id": "q2bI5aUygP2z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.vgg = vgg19(pretrained=True).features[:36].eval().to(DEVICE)\n",
        "    self.loss = nn.MSELoss()\n",
        "\n",
        "    for param in self.vgg.parameters():\n",
        "      param.requires_grad = False\n",
        "  \n",
        "  def forward(self, input, target):\n",
        "    vgg_input_features = self.vgg(input)\n",
        "    vgg_target_features = self.vgg(target)\n",
        "    return self.loss(vgg_input_features, vgg_target_features)"
      ],
      "metadata": {
        "id": "fnMrPGI_tySD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset(root_dir=TRAIN_PATH)\n",
        "loader = DataLoader(\n",
        "  dataset, \n",
        "  batch_size=BATCH_SIZE, \n",
        "  shuffle=True, \n",
        "  pin_memory=True, \n",
        "  num_workers=NUM_WORKERS\n",
        ")\n",
        "gen = Generator(in_channels=3, ratio=RATIO).to(DEVICE)\n",
        "disc = Discriminator(in_channels=3).to(DEVICE)\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "mse = nn.MSELoss()\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "vgg = VGGLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s5fkwfoOIyQ",
        "outputId": "51c177ca-7782-4d5b-d416-950f984b84f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(loader, disc, gen, opt_gen, opt_disc, mse, bce, vgg):\n",
        "  loop = tqdm(loader, leave=True)\n",
        "\n",
        "  for idx, (low_res, high_res) in enumerate(loop):\n",
        "    high_res = high_res.to(DEVICE)\n",
        "    low_res = low_res.to(DEVICE)\n",
        "        \n",
        "    fake = gen(low_res)\n",
        "    disc_real = disc(high_res)\n",
        "    disc_fake = disc(fake.detach())\n",
        "    disc_loss_real = bce(disc_real, torch.ones_like(disc_real) - 0.1*torch.rand_like(disc_real))\n",
        "    disc_loss_fake = bce(disc_fake, torch.zeros_like(disc_fake))\n",
        "    disc_loss = disc_loss_fake + disc_loss_real\n",
        "\n",
        "    opt_disc.zero_grad()\n",
        "    disc_loss.backward()\n",
        "    opt_disc.step()\n",
        "\n",
        "    disc_fake = disc(fake)\n",
        "    l2_loss = mse(fake, high_res)\n",
        "    adversarial_loss = bce(disc_fake, torch.ones_like(disc_fake))\n",
        "    loss_for_vgg = vgg(fake, high_res)\n",
        "    gen_loss = 6e-2*loss_for_vgg + 1e-2*adversarial_loss + 0.92*l2_loss\n",
        "\n",
        "    opt_gen.zero_grad()\n",
        "    gen_loss.backward()\n",
        "    opt_gen.step()\n",
        "\n",
        "    if not idx % 200:\n",
        "      plot_examples(VAL_PATH, gen)\n",
        "      print(f'Discrimantor loss:{disc_loss}')\n",
        "      print(f'Generative loss:{gen_loss}')"
      ],
      "metadata": {
        "id": "OXa0IHCDvIcJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "  load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE)\n",
        "  load_checkpoint(CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE)\n",
        "\n",
        "for epoch in range(START_EPOCHS-1,NUM_EPOCHS):\n",
        "  print(f'====================== EPOCH: {epoch+1} =====================')\n",
        "  train_fn(loader, disc, gen, opt_gen, opt_disc, mse, bce, vgg)\n",
        "\n",
        "  if SAVE_MODEL:\n",
        "    save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
        "    save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko7hWgq6ZfmI",
        "outputId": "c41ea64e-8810-476c-b945-6050e84dc97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================== EPOCH: 1 =====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/800 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}