{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isvCcQ8gV6i5"
      },
      "outputs": [],
      "source": [
        "!if [ ! -d \"./datasets/DIV2K_train_HR\" ]; then \\\n",
        "    wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip -P ./datasets && \\\n",
        "    unzip ./datasets/DIV2K_train_HR.zip -d ./datasets; \\\n",
        "fi\n",
        "\n",
        "!if [ ! -d \"./datasets/DIV2K_valid_LR_x8\" ]; then \\\n",
        "    wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_x8.zip -P ./datasets && \\\n",
        "    unzip ./datasets/DIV2K_valid_LR_x8.zip -d ./datasets; \\\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R3gfAQmT5WT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "import torch \n",
        "from math import log2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from torchvision.models import vgg19\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "from albumentations import Resize\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVl2ERPSTObL"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = './datasets/DIV2K_train_HR/'\n",
        "VAL_PATH = './datasets/DIV2K_valid_LR_x8/'\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = True\n",
        "CHECKPOINT_GEN = 'gen.pth.tar'\n",
        "CHECKPOINT_DISC = 'disc.pth.tar'\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "LEARNING_RATE = 1e-4\n",
        "START_EPOCHS = 1\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 0\n",
        "HIGH_RES = 256\n",
        "RATIO = 8\n",
        "LOW_RES = HIGH_RES // RATIO\n",
        "IMG_CHANNELS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDaRnrxbaVpM"
      },
      "outputs": [],
      "source": [
        "high_res_transform = A.Compose([\n",
        "  A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "  ToTensorV2(),\n",
        "])\n",
        "low_res_transform = A.Compose([\n",
        "  A.Resize(width=LOW_RES, height=LOW_RES, interpolation=Image.BICUBIC),\n",
        "  A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
        "  ToTensorV2(),\n",
        "])\n",
        "both_transforms = A.Compose([\n",
        "  A.RandomCrop(width=HIGH_RES, height=HIGH_RES),\n",
        "  A.HorizontalFlip(p=0.5),\n",
        "  A.RandomRotate90(p=0.5),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_4iuPuZ-qHE"
      },
      "outputs": [],
      "source": [
        "def gradient_penalty(critic, real, fake, device):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake.detach() * (1 - alpha)\n",
        "    interpolated_images.requires_grad_(True)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def plot_examples(low_res_folder, gen):\n",
        "    os.system(\"rm saved/*\")\n",
        "    files = os.listdir(low_res_folder)\n",
        "    np.random.shuffle(files)\n",
        "    gen.eval()\n",
        "    for file in files[:10]:\n",
        "        image = Image.open(low_res_folder + file)\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                upscaled_img = gen(\n",
        "                    test_transform(image=np.asarray(image))[\"image\"]\n",
        "                    .unsqueeze(0)\n",
        "                    .to(DEVICE)\n",
        "                )\n",
        "            save_image(upscaled_img * 0.5 + 0.5, f\"saved/{file}\")\n",
        "        except : print('Memory insufficient for that image')\n",
        "    gen.train()\n",
        "\n",
        "def compress(img: np.ndarray, ratio:int):\n",
        "    resized = Resize(width=img.shape[1]//ratio, height=img.shape[0]//ratio, interpolation=Image.BICUBIC)(image=img)[\"image\"]\n",
        "    return resized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhmrPnlhLems"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, root_dir):\n",
        "    super(MyDataset, self).__init__()\n",
        "    self.data = []\n",
        "    self.root_dir = root_dir\n",
        "    self.data = [os.path.join(root_dir, fl) for fl in os.listdir(root_dir)]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img_file = self.data[index]\n",
        "    image = np.array(Image.open(img_file))\n",
        "    image = both_transforms(image=image)['image']\n",
        "    high_res = high_res_transform(image=image)['image']\n",
        "    low_res = low_res_transform(image=image)['image']\n",
        "    return high_res, low_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2bI5aUygP2z"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(\n",
        "    self, \n",
        "    in_channels, \n",
        "    out_channels, \n",
        "    discriminator=False, \n",
        "    use_act=True, \n",
        "    use_bn=True, \n",
        "    **kwargs,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.use_act = use_act\n",
        "    self.cnn = nn.Conv2d(in_channels, out_channels, **kwargs, bias=(not use_bn))\n",
        "    self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
        "    self.act = (\n",
        "      nn.LeakyReLU(0.2, inplace=True) \n",
        "      if discriminator \n",
        "      else nn.PReLU(num_parameters=out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.cnn(x)\n",
        "    out = self.bn(out)\n",
        "    if self.use_act:\n",
        "      out = self.act(out)\n",
        "    return out\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super().__init__()\n",
        "    self.block1 = ConvBlock(\n",
        "      in_channels,\n",
        "      in_channels,\n",
        "      kernel_size=3,\n",
        "      stride=1,\n",
        "      padding=1,\n",
        "    )\n",
        "    self.block2 = ConvBlock(\n",
        "      in_channels,\n",
        "      in_channels,\n",
        "      kernel_size=3,\n",
        "      stride=1,\n",
        "      padding=1,\n",
        "      use_act=False,\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.block1(x)\n",
        "    out = self.block2(out)\n",
        "    return out+x\n",
        "\n",
        "class UnsampleBlock(nn.Module):\n",
        "  def __init__(self, in_channels, scale_factor):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(\n",
        "      in_channels,\n",
        "      in_channels * scale_factor**2,\n",
        "      kernel_size=3,\n",
        "      stride=1,\n",
        "      padding=1,\n",
        "    )\n",
        "    self.ps = nn.PixelShuffle(upscale_factor=scale_factor)\n",
        "    self.act = nn.PReLU(num_parameters=in_channels)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = self.ps(out)\n",
        "    out = self.act(out)\n",
        "    return out\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_channels=3, num_channels=64, num_blocks=16, ratio=4):\n",
        "    super().__init__()\n",
        "    self.initial = ConvBlock(\n",
        "      in_channels, \n",
        "      num_channels, \n",
        "      kernel_size=9, \n",
        "      stride=1, \n",
        "      padding=4, \n",
        "      use_bn=False\n",
        "    )\n",
        "    self.residuals = nn.Sequential(\n",
        "      *[ResidualBlock(num_channels) for _ in range(num_blocks)]\n",
        "    )\n",
        "    self.convblock = ConvBlock(\n",
        "      num_channels, \n",
        "      num_channels, \n",
        "      kernel_size=3, \n",
        "      stride=1, \n",
        "      padding=1, \n",
        "      use_act=False\n",
        "    )\n",
        "    self.unsamples = nn.Sequential(\n",
        "      *[UnsampleBlock(num_channels, 2) for _ in range(int(log2(ratio)))]\n",
        "    )\n",
        "    self.final = nn.Conv2d(\n",
        "      num_channels,\n",
        "      in_channels,\n",
        "      kernel_size=9,\n",
        "      stride=1,\n",
        "      padding=4\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    initial = self.initial(x)\n",
        "    x = self.residuals(initial)\n",
        "    x = self.convblock(x) + initial\n",
        "    x = self.unsamples(x)\n",
        "    return torch.tanh(self.final(x))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels=3, features=[64, 64, 128, 128, 256, 256, 512, 512]):\n",
        "    super().__init__()\n",
        "    blocks = []\n",
        "    for idx, feature in enumerate(features):\n",
        "      blocks.append(\n",
        "        ConvBlock(\n",
        "          in_channels, \n",
        "          feature, \n",
        "          kernel_size=3, \n",
        "          stride=(1 + idx%2), \n",
        "          padding=1, \n",
        "          discriminator=True,\n",
        "          use_act=True,\n",
        "          use_bn=idx\n",
        "        )\n",
        "      )\n",
        "      in_channels = feature\n",
        "    \n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.AdaptiveAvgPool2d((6, 6)),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(512*6*6, 1024),\n",
        "      nn.LeakyReLU(0.2, inplace=True),\n",
        "      nn.Linear(1024, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.blocks(x)\n",
        "    return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnMrPGI_tySD"
      },
      "outputs": [],
      "source": [
        "class VGGLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.vgg = vgg19(pretrained=True).features[:36].eval().to(DEVICE)\n",
        "    self.loss = nn.MSELoss()\n",
        "\n",
        "    for param in self.vgg.parameters():\n",
        "      param.requires_grad = False\n",
        "  \n",
        "  def forward(self, input, target):\n",
        "    vgg_input_features = self.vgg(input)\n",
        "    vgg_target_features = self.vgg(target)\n",
        "    return self.loss(vgg_input_features, vgg_target_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s5fkwfoOIyQ",
        "outputId": "51c177ca-7782-4d5b-d416-950f984b84f0"
      },
      "outputs": [],
      "source": [
        "dataset = MyDataset(root_dir=TRAIN_PATH)\n",
        "loader = DataLoader(\n",
        "  dataset, \n",
        "  batch_size=BATCH_SIZE, \n",
        "  shuffle=True, \n",
        "  pin_memory=True, \n",
        "  num_workers=NUM_WORKERS\n",
        ")\n",
        "gen = Generator(in_channels=3, ratio=RATIO).to(DEVICE)\n",
        "disc = Discriminator(in_channels=3).to(DEVICE)\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "mse = nn.MSELoss()\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "vgg = VGGLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXa0IHCDvIcJ"
      },
      "outputs": [],
      "source": [
        "def train_fn(loader, disc, gen, opt_gen, opt_disc, mse, bce, vgg):\n",
        "  loop = tqdm(loader, leave=True)\n",
        "\n",
        "  for idx, (low_res, high_res) in enumerate(loop):\n",
        "    high_res = high_res.to(DEVICE)\n",
        "    low_res = low_res.to(DEVICE)\n",
        "        \n",
        "    fake = gen(low_res)\n",
        "    disc_real = disc(high_res)\n",
        "    disc_fake = disc(fake.detach())\n",
        "    disc_loss_real = bce(disc_real, torch.ones_like(disc_real) - 0.1*torch.rand_like(disc_real))\n",
        "    disc_loss_fake = bce(disc_fake, torch.zeros_like(disc_fake))\n",
        "    disc_loss = disc_loss_fake + disc_loss_real\n",
        "\n",
        "    opt_disc.zero_grad()\n",
        "    disc_loss.backward()\n",
        "    opt_disc.step()\n",
        "\n",
        "    disc_fake = disc(fake)\n",
        "    l2_loss = mse(fake, high_res)\n",
        "    adversarial_loss = bce(disc_fake, torch.ones_like(disc_fake))\n",
        "    loss_for_vgg = vgg(fake, high_res)\n",
        "    gen_loss = 6e-2*loss_for_vgg + 1e-2*adversarial_loss + 0.92*l2_loss\n",
        "\n",
        "    opt_gen.zero_grad()\n",
        "    gen_loss.backward()\n",
        "    opt_gen.step()\n",
        "\n",
        "    if not idx % 200:\n",
        "      plot_examples(VAL_PATH, gen)\n",
        "      print(f'Discrimantor loss:{disc_loss}')\n",
        "      print(f'Generative loss:{gen_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko7hWgq6ZfmI",
        "outputId": "c41ea64e-8810-476c-b945-6050e84dc97e"
      },
      "outputs": [],
      "source": [
        "if LOAD_MODEL:\n",
        "  load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE)\n",
        "  load_checkpoint(CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE)\n",
        "\n",
        "for epoch in range(START_EPOCHS-1,NUM_EPOCHS):\n",
        "  print(f'====================== EPOCH: {epoch+1} =====================')\n",
        "  train_fn(loader, disc, gen, opt_gen, opt_disc, mse, bce, vgg)\n",
        "\n",
        "  if SAVE_MODEL:\n",
        "    save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
        "    save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
