{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TRAIN_DIR = './dataset/train/'\n",
    "TEST_DIR = './dataset/test/'\n",
    "MODEL_PATH = './model/'\n",
    "\n",
    "CLASSES = ['apple', 'banana', 'mango', 'orange']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_dir, mean=None, std=None, batch_size=32, shuffle=False):\n",
    "    transform_list = [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    "    if mean is not None and std is not None:\n",
    "        transform_list.append(transforms.Normalize(mean=mean, std=std))\n",
    "    \n",
    "    transform = transforms.Compose(transform_list)\n",
    "    dataset = ImageFolder(root=dataset_dir, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return dataset, loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.9134, 0.7984, 0.7936])\n",
      "Std: tensor([0.1263, 0.2862, 0.2772])\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader = create_dataset(\n",
    "    dataset_dir=TRAIN_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "data = next(iter(train_dataset))\n",
    "tensor = data[0].unsqueeze(0)\n",
    "mean = tensor.mean((0, 2, 3))\n",
    "std = tensor.std((0, 2, 3))\n",
    "\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')\n",
    "\n",
    "train_dataset, train_loader = create_dataset(\n",
    "    dataset_dir=TRAIN_DIR,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataset, test_loader = create_dataset(\n",
    "    dataset_dir=TEST_DIR,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(3, 16, 3)\n",
    "pool = nn.MaxPool2d(2, 2)\n",
    "conv2 = nn.Conv2d(16, 32, 5)\n",
    "conv3 = nn.Conv2d(32, 64, 5)\n",
    "inputs = torch.randn((1, 3, 224, 224))\n",
    "x = conv3(pool(conv2(pool(conv1(inputs)))))\n",
    "outputs = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ConvNet, self).__init__()\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv1 = nn.Conv2d(3, 64, 3)\n",
    "    self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "    self.fc1 = nn.Linear(128*54*54, 1024)\n",
    "    self.fc2 = nn.Linear(1024, NUM_CLASSES)\n",
    "\n",
    "  def forward(self, x):\n",
    "                                          # -> n, 4, 224, 224\n",
    "    x = self.pool(F.relu(self.conv1(x)))  # -> n, 32, 111, 111\n",
    "    x = self.pool(F.relu(self.conv2(x)))  # -> n, 32, 54, 54\n",
    "    x = x.view(-1, 128*54*54)              # -> n, 93321\n",
    "    x = F.relu(self.fc1(x))               # -> n, 128\n",
    "    x = self.fc2(x)                       # -> n, 4\n",
    "    return x\n",
    "  \n",
    "model = ConvNet().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Batch [1/20], Loss: 1.3653\n",
      "Epoch [1/50], Batch [2/20], Loss: 1.4977\n",
      "Epoch [1/50], Batch [3/20], Loss: 1.3155\n",
      "Epoch [1/50], Batch [4/20], Loss: 1.3894\n",
      "Epoch [1/50], Batch [5/20], Loss: 1.4674\n",
      "Epoch [1/50], Batch [6/20], Loss: 1.4483\n",
      "Epoch [1/50], Batch [7/20], Loss: 1.4347\n",
      "Epoch [1/50], Batch [8/20], Loss: 1.3474\n",
      "Epoch [1/50], Batch [9/20], Loss: 1.4296\n",
      "Epoch [1/50], Batch [10/20], Loss: 1.3290\n",
      "Epoch [1/50], Batch [11/20], Loss: 1.2629\n",
      "Epoch [1/50], Batch [12/20], Loss: 1.3783\n",
      "Epoch [1/50], Batch [13/20], Loss: 1.2607\n",
      "Epoch [1/50], Batch [14/20], Loss: 1.3732\n",
      "Epoch [1/50], Batch [15/20], Loss: 1.2752\n",
      "Epoch [1/50], Batch [16/20], Loss: 1.3102\n",
      "Epoch [1/50], Batch [17/20], Loss: 1.2859\n",
      "Epoch [1/50], Batch [18/20], Loss: 1.3018\n",
      "Epoch [1/50], Batch [19/20], Loss: 1.2888\n",
      "Epoch [1/50], Batch [20/20], Loss: 1.3221\n",
      "Epoch [2/50], Batch [1/20], Loss: 1.2428\n",
      "Epoch [2/50], Batch [2/20], Loss: 1.2586\n",
      "Epoch [2/50], Batch [3/20], Loss: 1.2386\n",
      "Epoch [2/50], Batch [4/20], Loss: 1.2100\n",
      "Epoch [2/50], Batch [5/20], Loss: 1.0969\n",
      "Epoch [2/50], Batch [6/20], Loss: 1.1973\n",
      "Epoch [2/50], Batch [7/20], Loss: 1.0637\n",
      "Epoch [2/50], Batch [8/20], Loss: 1.1610\n",
      "Epoch [2/50], Batch [9/20], Loss: 1.3449\n",
      "Epoch [2/50], Batch [10/20], Loss: 1.1722\n",
      "Epoch [2/50], Batch [11/20], Loss: 1.2316\n",
      "Epoch [2/50], Batch [12/20], Loss: 1.2619\n",
      "Epoch [2/50], Batch [13/20], Loss: 1.1180\n",
      "Epoch [2/50], Batch [14/20], Loss: 1.4696\n",
      "Epoch [2/50], Batch [15/20], Loss: 1.1713\n",
      "Epoch [2/50], Batch [16/20], Loss: 1.3623\n",
      "Epoch [2/50], Batch [17/20], Loss: 1.3316\n",
      "Epoch [2/50], Batch [18/20], Loss: 1.1224\n",
      "Epoch [2/50], Batch [19/20], Loss: 1.1465\n",
      "Epoch [2/50], Batch [20/20], Loss: 1.1060\n",
      "Epoch [3/50], Batch [1/20], Loss: 1.0324\n",
      "Epoch [3/50], Batch [2/20], Loss: 1.1379\n",
      "Epoch [3/50], Batch [3/20], Loss: 1.0411\n",
      "Epoch [3/50], Batch [4/20], Loss: 1.1478\n",
      "Epoch [3/50], Batch [5/20], Loss: 1.1287\n",
      "Epoch [3/50], Batch [6/20], Loss: 1.0880\n",
      "Epoch [3/50], Batch [7/20], Loss: 1.1293\n",
      "Epoch [3/50], Batch [8/20], Loss: 1.0654\n",
      "Epoch [3/50], Batch [9/20], Loss: 1.0637\n",
      "Epoch [3/50], Batch [10/20], Loss: 1.3057\n",
      "Epoch [3/50], Batch [11/20], Loss: 1.1779\n",
      "Epoch [3/50], Batch [12/20], Loss: 1.1202\n",
      "Epoch [3/50], Batch [13/20], Loss: 1.2680\n",
      "Epoch [3/50], Batch [14/20], Loss: 1.0633\n",
      "Epoch [3/50], Batch [15/20], Loss: 0.9851\n",
      "Epoch [3/50], Batch [16/20], Loss: 1.1470\n",
      "Epoch [3/50], Batch [17/20], Loss: 1.1265\n",
      "Epoch [3/50], Batch [18/20], Loss: 0.8842\n",
      "Epoch [3/50], Batch [19/20], Loss: 0.9211\n",
      "Epoch [3/50], Batch [20/20], Loss: 1.1349\n",
      "Epoch [4/50], Batch [1/20], Loss: 1.1278\n",
      "Epoch [4/50], Batch [2/20], Loss: 1.0636\n",
      "Epoch [4/50], Batch [3/20], Loss: 1.1335\n",
      "Epoch [4/50], Batch [4/20], Loss: 1.0186\n",
      "Epoch [4/50], Batch [5/20], Loss: 1.0103\n",
      "Epoch [4/50], Batch [6/20], Loss: 0.9342\n",
      "Epoch [4/50], Batch [7/20], Loss: 0.9944\n",
      "Epoch [4/50], Batch [8/20], Loss: 0.9851\n",
      "Epoch [4/50], Batch [9/20], Loss: 1.0158\n",
      "Epoch [4/50], Batch [10/20], Loss: 0.9581\n",
      "Epoch [4/50], Batch [11/20], Loss: 1.0543\n",
      "Epoch [4/50], Batch [12/20], Loss: 1.4051\n",
      "Epoch [4/50], Batch [13/20], Loss: 1.0392\n",
      "Epoch [4/50], Batch [14/20], Loss: 1.0924\n",
      "Epoch [4/50], Batch [15/20], Loss: 1.1505\n",
      "Epoch [4/50], Batch [16/20], Loss: 0.9191\n",
      "Epoch [4/50], Batch [17/20], Loss: 0.9275\n",
      "Epoch [4/50], Batch [18/20], Loss: 0.8977\n",
      "Epoch [4/50], Batch [19/20], Loss: 0.9591\n",
      "Epoch [4/50], Batch [20/20], Loss: 0.9292\n",
      "Epoch [5/50], Batch [1/20], Loss: 0.9535\n",
      "Epoch [5/50], Batch [2/20], Loss: 1.2770\n",
      "Epoch [5/50], Batch [3/20], Loss: 1.1001\n",
      "Epoch [5/50], Batch [4/20], Loss: 1.0203\n",
      "Epoch [5/50], Batch [5/20], Loss: 0.9244\n",
      "Epoch [5/50], Batch [6/20], Loss: 0.9247\n",
      "Epoch [5/50], Batch [7/20], Loss: 1.0608\n",
      "Epoch [5/50], Batch [8/20], Loss: 0.9141\n",
      "Epoch [5/50], Batch [9/20], Loss: 0.9859\n",
      "Epoch [5/50], Batch [10/20], Loss: 1.1443\n",
      "Epoch [5/50], Batch [11/20], Loss: 0.8266\n",
      "Epoch [5/50], Batch [12/20], Loss: 1.1895\n",
      "Epoch [5/50], Batch [13/20], Loss: 0.9932\n",
      "Epoch [5/50], Batch [14/20], Loss: 1.2167\n",
      "Epoch [5/50], Batch [15/20], Loss: 1.0416\n",
      "Epoch [5/50], Batch [16/20], Loss: 1.1934\n",
      "Epoch [5/50], Batch [17/20], Loss: 0.9086\n",
      "Epoch [5/50], Batch [18/20], Loss: 0.9199\n",
      "Epoch [5/50], Batch [19/20], Loss: 0.8261\n",
      "Epoch [5/50], Batch [20/20], Loss: 0.9934\n",
      "Epoch [6/50], Batch [1/20], Loss: 0.9096\n",
      "Epoch [6/50], Batch [2/20], Loss: 0.8810\n",
      "Epoch [6/50], Batch [3/20], Loss: 0.8573\n",
      "Epoch [6/50], Batch [4/20], Loss: 1.0585\n",
      "Epoch [6/50], Batch [5/20], Loss: 0.9586\n",
      "Epoch [6/50], Batch [6/20], Loss: 0.8260\n",
      "Epoch [6/50], Batch [7/20], Loss: 0.7866\n",
      "Epoch [6/50], Batch [8/20], Loss: 0.8846\n",
      "Epoch [6/50], Batch [9/20], Loss: 0.8813\n",
      "Epoch [6/50], Batch [10/20], Loss: 1.0509\n",
      "Epoch [6/50], Batch [11/20], Loss: 1.0701\n",
      "Epoch [6/50], Batch [12/20], Loss: 0.9176\n",
      "Epoch [6/50], Batch [13/20], Loss: 0.8714\n",
      "Epoch [6/50], Batch [14/20], Loss: 0.8240\n",
      "Epoch [6/50], Batch [15/20], Loss: 1.0167\n",
      "Epoch [6/50], Batch [16/20], Loss: 0.8683\n",
      "Epoch [6/50], Batch [17/20], Loss: 0.8273\n",
      "Epoch [6/50], Batch [18/20], Loss: 0.8672\n",
      "Epoch [6/50], Batch [19/20], Loss: 0.8358\n",
      "Epoch [6/50], Batch [20/20], Loss: 0.9226\n",
      "Epoch [7/50], Batch [1/20], Loss: 1.0353\n",
      "Epoch [7/50], Batch [2/20], Loss: 0.7734\n",
      "Epoch [7/50], Batch [3/20], Loss: 1.0058\n",
      "Epoch [7/50], Batch [4/20], Loss: 0.7894\n",
      "Epoch [7/50], Batch [5/20], Loss: 0.7576\n",
      "Epoch [7/50], Batch [6/20], Loss: 0.8936\n",
      "Epoch [7/50], Batch [7/20], Loss: 0.8405\n",
      "Epoch [7/50], Batch [8/20], Loss: 0.9044\n",
      "Epoch [7/50], Batch [9/20], Loss: 0.7456\n",
      "Epoch [7/50], Batch [10/20], Loss: 0.7885\n",
      "Epoch [7/50], Batch [11/20], Loss: 1.0114\n",
      "Epoch [7/50], Batch [12/20], Loss: 0.9425\n",
      "Epoch [7/50], Batch [13/20], Loss: 0.7940\n",
      "Epoch [7/50], Batch [14/20], Loss: 1.0034\n",
      "Epoch [7/50], Batch [15/20], Loss: 0.9338\n",
      "Epoch [7/50], Batch [16/20], Loss: 0.9255\n",
      "Epoch [7/50], Batch [17/20], Loss: 0.7415\n",
      "Epoch [7/50], Batch [18/20], Loss: 0.8229\n",
      "Epoch [7/50], Batch [19/20], Loss: 0.8390\n",
      "Epoch [7/50], Batch [20/20], Loss: 0.8765\n",
      "Epoch [8/50], Batch [1/20], Loss: 0.7908\n",
      "Epoch [8/50], Batch [2/20], Loss: 0.8448\n",
      "Epoch [8/50], Batch [3/20], Loss: 0.7866\n",
      "Epoch [8/50], Batch [4/20], Loss: 0.8088\n",
      "Epoch [8/50], Batch [5/20], Loss: 0.6924\n",
      "Epoch [8/50], Batch [6/20], Loss: 0.7858\n",
      "Epoch [8/50], Batch [7/20], Loss: 0.8249\n",
      "Epoch [8/50], Batch [8/20], Loss: 0.9050\n",
      "Epoch [8/50], Batch [9/20], Loss: 0.8068\n",
      "Epoch [8/50], Batch [10/20], Loss: 1.3040\n",
      "Epoch [8/50], Batch [11/20], Loss: 0.9912\n",
      "Epoch [8/50], Batch [12/20], Loss: 0.8262\n",
      "Epoch [8/50], Batch [13/20], Loss: 0.8439\n",
      "Epoch [8/50], Batch [14/20], Loss: 0.9738\n",
      "Epoch [8/50], Batch [15/20], Loss: 0.8385\n",
      "Epoch [8/50], Batch [16/20], Loss: 0.9426\n",
      "Epoch [8/50], Batch [17/20], Loss: 0.7795\n",
      "Epoch [8/50], Batch [18/20], Loss: 0.7052\n",
      "Epoch [8/50], Batch [19/20], Loss: 0.9287\n",
      "Epoch [8/50], Batch [20/20], Loss: 0.6861\n",
      "Epoch [9/50], Batch [1/20], Loss: 0.8002\n",
      "Epoch [9/50], Batch [2/20], Loss: 0.7551\n",
      "Epoch [9/50], Batch [3/20], Loss: 0.7097\n",
      "Epoch [9/50], Batch [4/20], Loss: 0.8132\n",
      "Epoch [9/50], Batch [5/20], Loss: 0.7353\n",
      "Epoch [9/50], Batch [6/20], Loss: 0.8302\n",
      "Epoch [9/50], Batch [7/20], Loss: 0.9232\n",
      "Epoch [9/50], Batch [8/20], Loss: 0.6587\n",
      "Epoch [9/50], Batch [9/20], Loss: 0.6818\n",
      "Epoch [9/50], Batch [10/20], Loss: 0.7209\n",
      "Epoch [9/50], Batch [11/20], Loss: 0.9127\n",
      "Epoch [9/50], Batch [12/20], Loss: 0.6976\n",
      "Epoch [9/50], Batch [13/20], Loss: 0.8022\n",
      "Epoch [9/50], Batch [14/20], Loss: 0.7422\n",
      "Epoch [9/50], Batch [15/20], Loss: 0.7936\n",
      "Epoch [9/50], Batch [16/20], Loss: 0.8075\n",
      "Epoch [9/50], Batch [17/20], Loss: 0.6895\n",
      "Epoch [9/50], Batch [18/20], Loss: 0.7901\n",
      "Epoch [9/50], Batch [19/20], Loss: 0.7527\n",
      "Epoch [9/50], Batch [20/20], Loss: 0.9107\n",
      "Epoch [10/50], Batch [1/20], Loss: 0.7727\n",
      "Epoch [10/50], Batch [2/20], Loss: 0.6148\n",
      "Epoch [10/50], Batch [3/20], Loss: 0.5502\n",
      "Epoch [10/50], Batch [4/20], Loss: 0.7678\n",
      "Epoch [10/50], Batch [5/20], Loss: 0.8266\n",
      "Epoch [10/50], Batch [6/20], Loss: 0.6043\n",
      "Epoch [10/50], Batch [7/20], Loss: 0.7816\n",
      "Epoch [10/50], Batch [8/20], Loss: 0.8003\n",
      "Epoch [10/50], Batch [9/20], Loss: 0.6975\n",
      "Epoch [10/50], Batch [10/20], Loss: 0.8577\n",
      "Epoch [10/50], Batch [11/20], Loss: 0.8069\n",
      "Epoch [10/50], Batch [12/20], Loss: 0.5685\n",
      "Epoch [10/50], Batch [13/20], Loss: 0.7079\n",
      "Epoch [10/50], Batch [14/20], Loss: 0.8532\n",
      "Epoch [10/50], Batch [15/20], Loss: 0.6639\n",
      "Epoch [10/50], Batch [16/20], Loss: 0.7347\n",
      "Epoch [10/50], Batch [17/20], Loss: 0.8111\n",
      "Epoch [10/50], Batch [18/20], Loss: 0.5792\n",
      "Epoch [10/50], Batch [19/20], Loss: 0.6822\n",
      "Epoch [10/50], Batch [20/20], Loss: 0.6702\n",
      "Epoch [11/50], Batch [1/20], Loss: 0.9331\n",
      "Epoch [11/50], Batch [2/20], Loss: 0.8241\n",
      "Epoch [11/50], Batch [3/20], Loss: 0.6514\n",
      "Epoch [11/50], Batch [4/20], Loss: 0.7642\n",
      "Epoch [11/50], Batch [5/20], Loss: 0.6300\n",
      "Epoch [11/50], Batch [6/20], Loss: 0.6095\n",
      "Epoch [11/50], Batch [7/20], Loss: 0.7443\n",
      "Epoch [11/50], Batch [8/20], Loss: 0.5538\n",
      "Epoch [11/50], Batch [9/20], Loss: 0.6310\n",
      "Epoch [11/50], Batch [10/20], Loss: 0.6291\n",
      "Epoch [11/50], Batch [11/20], Loss: 0.7778\n",
      "Epoch [11/50], Batch [12/20], Loss: 0.7444\n",
      "Epoch [11/50], Batch [13/20], Loss: 0.6942\n",
      "Epoch [11/50], Batch [14/20], Loss: 0.9110\n",
      "Epoch [11/50], Batch [15/20], Loss: 0.6722\n",
      "Epoch [11/50], Batch [16/20], Loss: 0.6897\n",
      "Epoch [11/50], Batch [17/20], Loss: 0.8289\n",
      "Epoch [11/50], Batch [18/20], Loss: 0.7127\n",
      "Epoch [11/50], Batch [19/20], Loss: 0.8197\n",
      "Epoch [11/50], Batch [20/20], Loss: 0.7558\n",
      "Epoch [12/50], Batch [1/20], Loss: 0.6502\n",
      "Epoch [12/50], Batch [2/20], Loss: 0.9037\n",
      "Epoch [12/50], Batch [3/20], Loss: 0.8818\n",
      "Epoch [12/50], Batch [4/20], Loss: 0.6887\n",
      "Epoch [12/50], Batch [5/20], Loss: 0.7918\n",
      "Epoch [12/50], Batch [6/20], Loss: 0.4196\n",
      "Epoch [12/50], Batch [7/20], Loss: 0.8103\n",
      "Epoch [12/50], Batch [8/20], Loss: 0.8351\n",
      "Epoch [12/50], Batch [9/20], Loss: 0.6337\n",
      "Epoch [12/50], Batch [10/20], Loss: 0.5309\n",
      "Epoch [12/50], Batch [11/20], Loss: 0.6924\n",
      "Epoch [12/50], Batch [12/20], Loss: 0.7999\n",
      "Epoch [12/50], Batch [13/20], Loss: 0.6786\n",
      "Epoch [12/50], Batch [14/20], Loss: 0.5199\n",
      "Epoch [12/50], Batch [15/20], Loss: 0.7177\n",
      "Epoch [12/50], Batch [16/20], Loss: 0.6921\n",
      "Epoch [12/50], Batch [17/20], Loss: 0.6353\n",
      "Epoch [12/50], Batch [18/20], Loss: 0.6352\n",
      "Epoch [12/50], Batch [19/20], Loss: 0.6236\n",
      "Epoch [12/50], Batch [20/20], Loss: 0.5745\n",
      "Epoch [13/50], Batch [1/20], Loss: 0.5771\n",
      "Epoch [13/50], Batch [2/20], Loss: 0.4920\n",
      "Epoch [13/50], Batch [3/20], Loss: 0.5432\n",
      "Epoch [13/50], Batch [4/20], Loss: 0.7639\n",
      "Epoch [13/50], Batch [5/20], Loss: 0.5532\n",
      "Epoch [13/50], Batch [6/20], Loss: 0.7343\n",
      "Epoch [13/50], Batch [7/20], Loss: 0.5657\n",
      "Epoch [13/50], Batch [8/20], Loss: 0.4893\n",
      "Epoch [13/50], Batch [9/20], Loss: 0.6123\n",
      "Epoch [13/50], Batch [10/20], Loss: 0.6719\n",
      "Epoch [13/50], Batch [11/20], Loss: 0.6737\n",
      "Epoch [13/50], Batch [12/20], Loss: 0.5674\n",
      "Epoch [13/50], Batch [13/20], Loss: 0.8472\n",
      "Epoch [13/50], Batch [14/20], Loss: 0.8215\n",
      "Epoch [13/50], Batch [15/20], Loss: 0.5814\n",
      "Epoch [13/50], Batch [16/20], Loss: 0.5162\n",
      "Epoch [13/50], Batch [17/20], Loss: 0.8825\n",
      "Epoch [13/50], Batch [18/20], Loss: 0.8178\n",
      "Epoch [13/50], Batch [19/20], Loss: 0.6484\n",
      "Epoch [13/50], Batch [20/20], Loss: 0.5916\n",
      "Epoch [14/50], Batch [1/20], Loss: 0.4805\n",
      "Epoch [14/50], Batch [2/20], Loss: 0.4268\n",
      "Epoch [14/50], Batch [3/20], Loss: 0.5138\n",
      "Epoch [14/50], Batch [4/20], Loss: 0.5534\n",
      "Epoch [14/50], Batch [5/20], Loss: 0.6446\n",
      "Epoch [14/50], Batch [6/20], Loss: 0.7099\n",
      "Epoch [14/50], Batch [7/20], Loss: 0.9242\n",
      "Epoch [14/50], Batch [8/20], Loss: 0.7926\n",
      "Epoch [14/50], Batch [9/20], Loss: 0.6799\n",
      "Epoch [14/50], Batch [10/20], Loss: 0.9275\n",
      "Epoch [14/50], Batch [11/20], Loss: 0.5261\n",
      "Epoch [14/50], Batch [12/20], Loss: 0.6311\n",
      "Epoch [14/50], Batch [13/20], Loss: 0.6822\n",
      "Epoch [14/50], Batch [14/20], Loss: 0.6052\n",
      "Epoch [14/50], Batch [15/20], Loss: 0.6320\n",
      "Epoch [14/50], Batch [16/20], Loss: 0.7372\n",
      "Epoch [14/50], Batch [17/20], Loss: 0.6094\n",
      "Epoch [14/50], Batch [18/20], Loss: 0.5828\n",
      "Epoch [14/50], Batch [19/20], Loss: 0.4806\n",
      "Epoch [14/50], Batch [20/20], Loss: 0.5721\n",
      "Epoch [15/50], Batch [1/20], Loss: 0.4696\n",
      "Epoch [15/50], Batch [2/20], Loss: 0.5318\n",
      "Epoch [15/50], Batch [3/20], Loss: 0.6054\n",
      "Epoch [15/50], Batch [4/20], Loss: 0.6037\n",
      "Epoch [15/50], Batch [5/20], Loss: 0.7144\n",
      "Epoch [15/50], Batch [6/20], Loss: 0.6429\n",
      "Epoch [15/50], Batch [7/20], Loss: 0.6435\n",
      "Epoch [15/50], Batch [8/20], Loss: 0.6285\n",
      "Epoch [15/50], Batch [9/20], Loss: 0.5660\n",
      "Epoch [15/50], Batch [10/20], Loss: 0.6214\n",
      "Epoch [15/50], Batch [11/20], Loss: 0.6681\n",
      "Epoch [15/50], Batch [12/20], Loss: 0.7114\n",
      "Epoch [15/50], Batch [13/20], Loss: 0.4593\n",
      "Epoch [15/50], Batch [14/20], Loss: 0.5602\n",
      "Epoch [15/50], Batch [15/20], Loss: 0.4799\n",
      "Epoch [15/50], Batch [16/20], Loss: 0.6253\n",
      "Epoch [15/50], Batch [17/20], Loss: 0.6190\n",
      "Epoch [15/50], Batch [18/20], Loss: 0.6827\n",
      "Epoch [15/50], Batch [19/20], Loss: 0.4918\n",
      "Epoch [15/50], Batch [20/20], Loss: 0.5543\n",
      "Epoch [16/50], Batch [1/20], Loss: 0.6626\n",
      "Epoch [16/50], Batch [2/20], Loss: 0.4774\n",
      "Epoch [16/50], Batch [3/20], Loss: 0.6084\n",
      "Epoch [16/50], Batch [4/20], Loss: 0.5248\n",
      "Epoch [16/50], Batch [5/20], Loss: 0.4872\n",
      "Epoch [16/50], Batch [6/20], Loss: 0.6524\n",
      "Epoch [16/50], Batch [7/20], Loss: 0.3976\n",
      "Epoch [16/50], Batch [8/20], Loss: 0.3934\n",
      "Epoch [16/50], Batch [9/20], Loss: 0.4023\n",
      "Epoch [16/50], Batch [10/20], Loss: 0.6517\n",
      "Epoch [16/50], Batch [11/20], Loss: 0.4691\n",
      "Epoch [16/50], Batch [12/20], Loss: 0.5840\n",
      "Epoch [16/50], Batch [13/20], Loss: 0.5667\n",
      "Epoch [16/50], Batch [14/20], Loss: 0.5466\n",
      "Epoch [16/50], Batch [15/20], Loss: 0.5513\n",
      "Epoch [16/50], Batch [16/20], Loss: 0.5244\n",
      "Epoch [16/50], Batch [17/20], Loss: 0.5568\n",
      "Epoch [16/50], Batch [18/20], Loss: 0.6933\n",
      "Epoch [16/50], Batch [19/20], Loss: 0.9148\n",
      "Epoch [16/50], Batch [20/20], Loss: 0.6782\n",
      "Epoch [17/50], Batch [1/20], Loss: 0.6498\n",
      "Epoch [17/50], Batch [2/20], Loss: 0.4003\n",
      "Epoch [17/50], Batch [3/20], Loss: 0.3396\n",
      "Epoch [17/50], Batch [4/20], Loss: 0.7050\n",
      "Epoch [17/50], Batch [5/20], Loss: 0.5564\n",
      "Epoch [17/50], Batch [6/20], Loss: 0.4761\n",
      "Epoch [17/50], Batch [7/20], Loss: 0.6001\n",
      "Epoch [17/50], Batch [8/20], Loss: 0.5684\n",
      "Epoch [17/50], Batch [9/20], Loss: 0.5569\n",
      "Epoch [17/50], Batch [10/20], Loss: 0.6062\n",
      "Epoch [17/50], Batch [11/20], Loss: 0.6259\n",
      "Epoch [17/50], Batch [12/20], Loss: 0.4723\n",
      "Epoch [17/50], Batch [13/20], Loss: 0.5642\n",
      "Epoch [17/50], Batch [14/20], Loss: 0.3973\n",
      "Epoch [17/50], Batch [15/20], Loss: 0.4274\n",
      "Epoch [17/50], Batch [16/20], Loss: 0.5877\n",
      "Epoch [17/50], Batch [17/20], Loss: 0.7871\n",
      "Epoch [17/50], Batch [18/20], Loss: 0.4973\n",
      "Epoch [17/50], Batch [19/20], Loss: 0.5009\n",
      "Epoch [17/50], Batch [20/20], Loss: 0.7216\n",
      "Epoch [18/50], Batch [1/20], Loss: 0.4344\n",
      "Epoch [18/50], Batch [2/20], Loss: 0.4654\n",
      "Epoch [18/50], Batch [3/20], Loss: 0.3514\n",
      "Epoch [18/50], Batch [4/20], Loss: 0.5663\n",
      "Epoch [18/50], Batch [5/20], Loss: 0.4969\n",
      "Epoch [18/50], Batch [6/20], Loss: 0.5506\n",
      "Epoch [18/50], Batch [7/20], Loss: 0.5275\n",
      "Epoch [18/50], Batch [8/20], Loss: 0.5994\n",
      "Epoch [18/50], Batch [9/20], Loss: 0.7061\n",
      "Epoch [18/50], Batch [10/20], Loss: 0.6465\n",
      "Epoch [18/50], Batch [11/20], Loss: 0.6528\n",
      "Epoch [18/50], Batch [12/20], Loss: 0.4986\n",
      "Epoch [18/50], Batch [13/20], Loss: 0.7609\n",
      "Epoch [18/50], Batch [14/20], Loss: 0.4949\n",
      "Epoch [18/50], Batch [15/20], Loss: 0.7618\n",
      "Epoch [18/50], Batch [16/20], Loss: 0.4847\n",
      "Epoch [18/50], Batch [17/20], Loss: 0.4476\n",
      "Epoch [18/50], Batch [18/20], Loss: 0.5157\n",
      "Epoch [18/50], Batch [19/20], Loss: 0.5438\n",
      "Epoch [18/50], Batch [20/20], Loss: 0.4338\n",
      "Epoch [19/50], Batch [1/20], Loss: 0.3685\n",
      "Epoch [19/50], Batch [2/20], Loss: 0.4475\n",
      "Epoch [19/50], Batch [3/20], Loss: 0.4530\n",
      "Epoch [19/50], Batch [4/20], Loss: 0.4294\n",
      "Epoch [19/50], Batch [5/20], Loss: 0.3861\n",
      "Epoch [19/50], Batch [6/20], Loss: 0.4700\n",
      "Epoch [19/50], Batch [7/20], Loss: 0.4637\n",
      "Epoch [19/50], Batch [8/20], Loss: 0.4067\n",
      "Epoch [19/50], Batch [9/20], Loss: 0.6014\n",
      "Epoch [19/50], Batch [10/20], Loss: 0.5868\n",
      "Epoch [19/50], Batch [11/20], Loss: 0.3103\n",
      "Epoch [19/50], Batch [12/20], Loss: 0.6207\n",
      "Epoch [19/50], Batch [13/20], Loss: 0.3762\n",
      "Epoch [19/50], Batch [14/20], Loss: 0.4541\n",
      "Epoch [19/50], Batch [15/20], Loss: 0.4938\n",
      "Epoch [19/50], Batch [16/20], Loss: 0.4878\n",
      "Epoch [19/50], Batch [17/20], Loss: 0.3294\n",
      "Epoch [19/50], Batch [18/20], Loss: 0.5281\n",
      "Epoch [19/50], Batch [19/20], Loss: 0.5734\n",
      "Epoch [19/50], Batch [20/20], Loss: 0.7995\n",
      "Epoch [20/50], Batch [1/20], Loss: 0.6595\n",
      "Epoch [20/50], Batch [2/20], Loss: 0.4503\n",
      "Epoch [20/50], Batch [3/20], Loss: 0.3901\n",
      "Epoch [20/50], Batch [4/20], Loss: 0.4494\n",
      "Epoch [20/50], Batch [5/20], Loss: 0.4526\n",
      "Epoch [20/50], Batch [6/20], Loss: 0.3650\n",
      "Epoch [20/50], Batch [7/20], Loss: 0.5193\n",
      "Epoch [20/50], Batch [8/20], Loss: 0.5082\n",
      "Epoch [20/50], Batch [9/20], Loss: 0.4456\n",
      "Epoch [20/50], Batch [10/20], Loss: 0.9337\n",
      "Epoch [20/50], Batch [11/20], Loss: 0.9724\n",
      "Epoch [20/50], Batch [12/20], Loss: 0.4762\n",
      "Epoch [20/50], Batch [13/20], Loss: 0.8278\n",
      "Epoch [20/50], Batch [14/20], Loss: 0.5660\n",
      "Epoch [20/50], Batch [15/20], Loss: 0.3528\n",
      "Epoch [20/50], Batch [16/20], Loss: 0.4615\n",
      "Epoch [20/50], Batch [17/20], Loss: 0.4383\n",
      "Epoch [20/50], Batch [18/20], Loss: 0.5497\n",
      "Epoch [20/50], Batch [19/20], Loss: 0.5114\n",
      "Epoch [20/50], Batch [20/20], Loss: 0.5523\n",
      "Epoch [21/50], Batch [1/20], Loss: 0.6322\n",
      "Epoch [21/50], Batch [2/20], Loss: 0.5956\n",
      "Epoch [21/50], Batch [3/20], Loss: 0.3460\n",
      "Epoch [21/50], Batch [4/20], Loss: 0.4908\n",
      "Epoch [21/50], Batch [5/20], Loss: 0.4350\n",
      "Epoch [21/50], Batch [6/20], Loss: 0.3483\n",
      "Epoch [21/50], Batch [7/20], Loss: 0.5086\n",
      "Epoch [21/50], Batch [8/20], Loss: 0.4792\n",
      "Epoch [21/50], Batch [9/20], Loss: 0.6172\n",
      "Epoch [21/50], Batch [10/20], Loss: 0.6259\n",
      "Epoch [21/50], Batch [11/20], Loss: 0.7290\n",
      "Epoch [21/50], Batch [12/20], Loss: 0.3277\n",
      "Epoch [21/50], Batch [13/20], Loss: 0.3951\n",
      "Epoch [21/50], Batch [14/20], Loss: 0.4877\n",
      "Epoch [21/50], Batch [15/20], Loss: 0.4625\n",
      "Epoch [21/50], Batch [16/20], Loss: 0.3821\n",
      "Epoch [21/50], Batch [17/20], Loss: 0.4810\n",
      "Epoch [21/50], Batch [18/20], Loss: 0.5365\n",
      "Epoch [21/50], Batch [19/20], Loss: 0.5383\n",
      "Epoch [21/50], Batch [20/20], Loss: 0.4102\n",
      "Epoch [22/50], Batch [1/20], Loss: 0.4828\n",
      "Epoch [22/50], Batch [2/20], Loss: 0.2698\n",
      "Epoch [22/50], Batch [3/20], Loss: 0.3218\n",
      "Epoch [22/50], Batch [4/20], Loss: 0.4775\n",
      "Epoch [22/50], Batch [5/20], Loss: 0.4595\n",
      "Epoch [22/50], Batch [6/20], Loss: 0.3048\n",
      "Epoch [22/50], Batch [7/20], Loss: 0.4334\n",
      "Epoch [22/50], Batch [8/20], Loss: 0.3376\n",
      "Epoch [22/50], Batch [9/20], Loss: 0.3257\n",
      "Epoch [22/50], Batch [10/20], Loss: 0.3164\n",
      "Epoch [22/50], Batch [11/20], Loss: 0.7646\n",
      "Epoch [22/50], Batch [12/20], Loss: 0.7931\n",
      "Epoch [22/50], Batch [13/20], Loss: 0.6084\n",
      "Epoch [22/50], Batch [14/20], Loss: 1.0247\n",
      "Epoch [22/50], Batch [15/20], Loss: 0.5025\n",
      "Epoch [22/50], Batch [16/20], Loss: 0.3073\n",
      "Epoch [22/50], Batch [17/20], Loss: 0.3662\n",
      "Epoch [22/50], Batch [18/20], Loss: 0.4142\n",
      "Epoch [22/50], Batch [19/20], Loss: 0.4196\n",
      "Epoch [22/50], Batch [20/20], Loss: 0.4065\n",
      "Epoch [23/50], Batch [1/20], Loss: 0.3243\n",
      "Epoch [23/50], Batch [2/20], Loss: 0.2995\n",
      "Epoch [23/50], Batch [3/20], Loss: 0.4032\n",
      "Epoch [23/50], Batch [4/20], Loss: 0.4721\n",
      "Epoch [23/50], Batch [5/20], Loss: 0.3779\n",
      "Epoch [23/50], Batch [6/20], Loss: 0.2653\n",
      "Epoch [23/50], Batch [7/20], Loss: 0.4602\n",
      "Epoch [23/50], Batch [8/20], Loss: 0.4744\n",
      "Epoch [23/50], Batch [9/20], Loss: 0.4407\n",
      "Epoch [23/50], Batch [10/20], Loss: 0.4287\n",
      "Epoch [23/50], Batch [11/20], Loss: 0.4579\n",
      "Epoch [23/50], Batch [12/20], Loss: 0.4122\n",
      "Epoch [23/50], Batch [13/20], Loss: 0.6507\n",
      "Epoch [23/50], Batch [14/20], Loss: 0.6554\n",
      "Epoch [23/50], Batch [15/20], Loss: 0.5228\n",
      "Epoch [23/50], Batch [16/20], Loss: 0.4378\n",
      "Epoch [23/50], Batch [17/20], Loss: 0.3174\n",
      "Epoch [23/50], Batch [18/20], Loss: 0.3786\n",
      "Epoch [23/50], Batch [19/20], Loss: 0.5121\n",
      "Epoch [23/50], Batch [20/20], Loss: 0.2601\n",
      "Epoch [24/50], Batch [1/20], Loss: 0.3843\n",
      "Epoch [24/50], Batch [2/20], Loss: 0.4191\n",
      "Epoch [24/50], Batch [3/20], Loss: 0.4047\n",
      "Epoch [24/50], Batch [4/20], Loss: 0.2915\n",
      "Epoch [24/50], Batch [5/20], Loss: 0.2796\n",
      "Epoch [24/50], Batch [6/20], Loss: 0.2845\n",
      "Epoch [24/50], Batch [7/20], Loss: 0.2531\n",
      "Epoch [24/50], Batch [8/20], Loss: 0.3796\n",
      "Epoch [24/50], Batch [9/20], Loss: 0.3116\n",
      "Epoch [24/50], Batch [10/20], Loss: 0.3295\n",
      "Epoch [24/50], Batch [11/20], Loss: 0.4048\n",
      "Epoch [24/50], Batch [12/20], Loss: 0.3717\n",
      "Epoch [24/50], Batch [13/20], Loss: 0.4032\n",
      "Epoch [24/50], Batch [14/20], Loss: 0.5701\n",
      "Epoch [24/50], Batch [15/20], Loss: 0.3040\n",
      "Epoch [24/50], Batch [16/20], Loss: 0.3649\n",
      "Epoch [24/50], Batch [17/20], Loss: 0.2738\n",
      "Epoch [24/50], Batch [18/20], Loss: 0.5848\n",
      "Epoch [24/50], Batch [19/20], Loss: 0.6114\n",
      "Epoch [24/50], Batch [20/20], Loss: 0.4732\n",
      "Epoch [25/50], Batch [1/20], Loss: 0.3762\n",
      "Epoch [25/50], Batch [2/20], Loss: 0.3897\n",
      "Epoch [25/50], Batch [3/20], Loss: 0.3997\n",
      "Epoch [25/50], Batch [4/20], Loss: 0.4418\n",
      "Epoch [25/50], Batch [5/20], Loss: 0.2779\n",
      "Epoch [25/50], Batch [6/20], Loss: 0.3527\n",
      "Epoch [25/50], Batch [7/20], Loss: 0.2500\n",
      "Epoch [25/50], Batch [8/20], Loss: 0.2541\n",
      "Epoch [25/50], Batch [9/20], Loss: 0.3280\n",
      "Epoch [25/50], Batch [10/20], Loss: 0.3215\n",
      "Epoch [25/50], Batch [11/20], Loss: 0.4588\n",
      "Epoch [25/50], Batch [12/20], Loss: 0.4987\n",
      "Epoch [25/50], Batch [13/20], Loss: 0.2951\n",
      "Epoch [25/50], Batch [14/20], Loss: 0.2584\n",
      "Epoch [25/50], Batch [15/20], Loss: 0.2513\n",
      "Epoch [25/50], Batch [16/20], Loss: 0.2409\n",
      "Epoch [25/50], Batch [17/20], Loss: 0.2682\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b299881f6006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{i+1}/{num_batches}], Loss: {loss.item():.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py3.9/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py3.9/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py3.9/lib/python3.9/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    149\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             sgd(params_with_grad,\n\u001b[0m\u001b[1;32m    152\u001b[0m                 \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py3.9/lib/python3.9/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    203\u001b[0m          \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m          \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/py3.9/lib/python3.9/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_batches = len(train_loader)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  for i, (images, labels) in enumerate(train_loader):\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    # Forward\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print (f'Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{i+1}/{num_batches}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-251083430be9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m torch.save(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34mf'{MODEL_PATH}custom_cnn_fruit_dataset_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")}.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(\n",
    "  model.state_dict(), \n",
    "  f'{MODEL_PATH}custom_cnn_fruit_dataset_{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")}.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 52.77777777777778 %\n",
      "Accuracy of apple: 100.0 %\n",
      "Accuracy of banana: 100.0 %\n",
      "Accuracy of mango: 75.0 %\n",
      "Accuracy of orange: 66.66666666666667 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  n_correct = 0\n",
    "  n_samples = 0\n",
    "  n_class_correct = [0 for i in range(10)]\n",
    "  n_class_samples = [0 for i in range(10)]\n",
    "  \n",
    "  for images, labels in test_loader:\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    n_samples += labels.size(0)\n",
    "    n_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    for i in range(len(test_loader)):\n",
    "      label = labels[i]\n",
    "      pred = predicted[i]\n",
    "      if (label == pred):\n",
    "        n_class_correct[label] += 1\n",
    "      n_class_samples[label] += 1\n",
    "    \n",
    "  acc = 100.0 * n_correct / n_samples\n",
    "  print(f'Accuracy of the network: {acc} %')\n",
    "  \n",
    "  for i in range(NUM_CLASSES):\n",
    "    acc = 0\n",
    "    if n_class_samples[i] != 0:\n",
    "      acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "    print(f'Accuracy of {CLASSES[i]}: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
